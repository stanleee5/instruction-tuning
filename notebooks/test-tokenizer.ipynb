{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Template \n",
    "- instruction-tuning 템플릿으로 변환했을 때 masking 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='JackFram/llama-160m', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from colorama import Fore\n",
    "from icecream import ic\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "MODEL_DIR = \"/home/hyunsoo.lee/disk1/llm-models/llama-160m\"\n",
    "MODEL_DIR = \"/home/hyunsoo.lee/disk1/llm-models/CodeLlama-13b-hf\"\n",
    "MODEL_DIR = \"/home/hyunsoo.lee/disk1/llm-models/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(tokenizer.eos_token)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(instruction_template, instruction, response_template, response):\n",
    "    return instruction_template + instruction.strip() + response_template + response\n",
    "\n",
    "\n",
    "def is_included(s, target):\n",
    "    for i in range(len(s)):\n",
    "        cnt = 0\n",
    "        for si, ti in zip(s[i:], target):\n",
    "            if si != ti:\n",
    "                break\n",
    "            cnt += 1\n",
    "        if cnt == len(target):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check(instruction_template, instruction, response_template, response):\n",
    "    input_text = merge(instruction_template, instruction, response_template, response)\n",
    "    print(Fore.WHITE + input_text)\n",
    "\n",
    "    input_text_tokenized = tokenizer.encode(input_text)\n",
    "    merged = [\n",
    "        tokenizer.encode(x, add_special_tokens=False)\n",
    "        for x in [instruction_template, instruction, response_template, response]\n",
    "    ]\n",
    "    print(Fore.YELLOW + \" + \".join(tokenizer.decode(x) for x in input_text_tokenized))\n",
    "\n",
    "    instruction_template_tokenized = tokenizer.encode(\n",
    "        instruction_template, add_special_tokens=False\n",
    "    )\n",
    "    response_template_tokenized = tokenizer.encode(\n",
    "        response_template, add_special_tokens=False\n",
    "    )[1:]\n",
    "    # print(Fore.WHITE)\n",
    "    # print(instruction_template_tokenized)\n",
    "    # print(\" + \".join(tokenizer.decode(x) for x in instruction_template_tokenized))\n",
    "    # print(response_template_tokenized)\n",
    "    # print(\" + \".join(tokenizer.decode(x) for x in response_template_tokenized))\n",
    "\n",
    "    print(Fore.GREEN)\n",
    "    i_included = is_included(input_text_tokenized, instruction_template_tokenized)\n",
    "    o_included = is_included(input_text_tokenized, response_template_tokenized)\n",
    "    print(f\"instruction_template_included: {i_included}\")\n",
    "    print(f\"response_template_included: {o_included}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m### Question:\n",
      "your name?\n",
      "\n",
      "### Answer:\n",
      "guy\n",
      "\u001b[33m<s> + ### + Question + : + \n",
      " + your + name + ? + \n",
      " + \n",
      " + ## + # + Answer + : + \n",
      " + gu + y\n",
      "\u001b[32m\n",
      "instruction_template_included: True\n",
      "response_template_included: True\n",
      "\n",
      "\u001b[37mQuestion:\n",
      "your name?\n",
      "Answer:\n",
      "guy\n",
      "\u001b[33m<s> + Question + : + \n",
      " + your + name + ? + \n",
      " + Answer + : + \n",
      " + gu + y\n",
      "\u001b[32m\n",
      "instruction_template_included: True\n",
      "response_template_included: True\n",
      "\n",
      "\u001b[37m[INST]\n",
      "your name?\n",
      "[/INST]\n",
      "guy\n",
      "\u001b[33m<s> + [ + INST + ] + \n",
      " + your + name + ? + \n",
      " + [ + / + INST + ] + \n",
      " + gu + y\n",
      "\u001b[32m\n",
      "instruction_template_included: True\n",
      "response_template_included: True\n",
      "\n",
      "\u001b[37m[INST] your name? [/INST]guy\n",
      "\u001b[33m<s> + [ + INST + ] + your + name + ? + [ + / + INST + ] + gu + y\n",
      "\u001b[32m\n",
      "instruction_template_included: False\n",
      "response_template_included: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"your name?\"\n",
    "response = \"guy\"\n",
    "\n",
    "templates = [\n",
    "    (\"### Question:\\n\", \"\\n\\n### Answer:\\n\"),\n",
    "    (\"Question:\\n\", \"\\nAnswer:\\n\"),\n",
    "    (\"[INST]\\n\", \"\\n[/INST]\\n\"),\n",
    "    (\"[INST] \", \" [/INST]\"),\n",
    "]\n",
    "\n",
    "for instruction_template, response_template in templates:\n",
    "    check(instruction_template, instruction, response_template, response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
